Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 16, 16]           9,408
       BatchNorm2d-2           [-1, 64, 16, 16]             128
              ReLU-3           [-1, 64, 16, 16]               0
         MaxPool2d-4             [-1, 64, 8, 8]               0
            Conv2d-5             [-1, 64, 8, 8]          36,864
       BatchNorm2d-6             [-1, 64, 8, 8]             128
              ReLU-7             [-1, 64, 8, 8]               0
            Conv2d-8             [-1, 64, 8, 8]          36,864
       BatchNorm2d-9             [-1, 64, 8, 8]             128
             ReLU-10             [-1, 64, 8, 8]               0
       BasicBlock-11             [-1, 64, 8, 8]               0
           Conv2d-12             [-1, 64, 8, 8]          36,864
      BatchNorm2d-13             [-1, 64, 8, 8]             128
             ReLU-14             [-1, 64, 8, 8]               0
           Conv2d-15             [-1, 64, 8, 8]          36,864
      BatchNorm2d-16             [-1, 64, 8, 8]             128
             ReLU-17             [-1, 64, 8, 8]               0
       BasicBlock-18             [-1, 64, 8, 8]               0
           Conv2d-19            [-1, 128, 4, 4]          73,728
      BatchNorm2d-20            [-1, 128, 4, 4]             256
             ReLU-21            [-1, 128, 4, 4]               0
           Conv2d-22            [-1, 128, 4, 4]         147,456
      BatchNorm2d-23            [-1, 128, 4, 4]             256
           Conv2d-24            [-1, 128, 4, 4]           8,192
      BatchNorm2d-25            [-1, 128, 4, 4]             256
             ReLU-26            [-1, 128, 4, 4]               0
       BasicBlock-27            [-1, 128, 4, 4]               0
           Conv2d-28            [-1, 128, 4, 4]         147,456
      BatchNorm2d-29            [-1, 128, 4, 4]             256
             ReLU-30            [-1, 128, 4, 4]               0
           Conv2d-31            [-1, 128, 4, 4]         147,456
      BatchNorm2d-32            [-1, 128, 4, 4]             256
             ReLU-33            [-1, 128, 4, 4]               0
       BasicBlock-34            [-1, 128, 4, 4]               0
           Conv2d-35            [-1, 256, 2, 2]         294,912
      BatchNorm2d-36            [-1, 256, 2, 2]             512
             ReLU-37            [-1, 256, 2, 2]               0
           Conv2d-38            [-1, 256, 2, 2]         589,824
      BatchNorm2d-39            [-1, 256, 2, 2]             512
           Conv2d-40            [-1, 256, 2, 2]          32,768
      BatchNorm2d-41            [-1, 256, 2, 2]             512
             ReLU-42            [-1, 256, 2, 2]               0
       BasicBlock-43            [-1, 256, 2, 2]               0
           Conv2d-44            [-1, 256, 2, 2]         589,824
      BatchNorm2d-45            [-1, 256, 2, 2]             512
             ReLU-46            [-1, 256, 2, 2]               0
           Conv2d-47            [-1, 256, 2, 2]         589,824
      BatchNorm2d-48            [-1, 256, 2, 2]             512
             ReLU-49            [-1, 256, 2, 2]               0
       BasicBlock-50            [-1, 256, 2, 2]               0
           Conv2d-51            [-1, 512, 1, 1]       1,179,648
      BatchNorm2d-52            [-1, 512, 1, 1]           1,024
             ReLU-53            [-1, 512, 1, 1]               0
           Conv2d-54            [-1, 512, 1, 1]       2,359,296
      BatchNorm2d-55            [-1, 512, 1, 1]           1,024
           Conv2d-56            [-1, 512, 1, 1]         131,072
      BatchNorm2d-57            [-1, 512, 1, 1]           1,024
             ReLU-58            [-1, 512, 1, 1]               0
       BasicBlock-59            [-1, 512, 1, 1]               0
           Conv2d-60            [-1, 512, 1, 1]       2,359,296
      BatchNorm2d-61            [-1, 512, 1, 1]           1,024
             ReLU-62            [-1, 512, 1, 1]               0
           Conv2d-63            [-1, 512, 1, 1]       2,359,296
      BatchNorm2d-64            [-1, 512, 1, 1]           1,024
             ReLU-65            [-1, 512, 1, 1]               0
       BasicBlock-66            [-1, 512, 1, 1]               0
AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,181,642
Trainable params: 11,181,642
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 1.29
Params size (MB): 42.65
Estimated Total Size (MB): 43.95
----------------------------------------------------------------
None
New trainable parameters are 4924990
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 20, 32, 32]           2,940
       BatchNorm2d-2           [-1, 20, 32, 32]              40
              ReLU-3           [-1, 20, 32, 32]               0
         MaxPool2d-4           [-1, 20, 32, 32]               0
            Conv2d-5           [-1, 40, 16, 16]           7,200
       BatchNorm2d-6           [-1, 40, 16, 16]              80
              ReLU-7           [-1, 40, 16, 16]               0
            Conv2d-8           [-1, 40, 16, 16]          14,400
       BatchNorm2d-9           [-1, 40, 16, 16]              80
           Conv2d-10           [-1, 40, 16, 16]             800
      BatchNorm2d-11           [-1, 40, 16, 16]              80
             ReLU-12           [-1, 40, 16, 16]               0
       BasicBlock-13           [-1, 40, 16, 16]               0
           Conv2d-14           [-1, 40, 16, 16]          14,400
      BatchNorm2d-15           [-1, 40, 16, 16]              80
             ReLU-16           [-1, 40, 16, 16]               0
           Conv2d-17           [-1, 40, 16, 16]          14,400
      BatchNorm2d-18           [-1, 40, 16, 16]              80
             ReLU-19           [-1, 40, 16, 16]               0
       BasicBlock-20           [-1, 40, 16, 16]               0
           Conv2d-21           [-1, 40, 16, 16]          14,400
      BatchNorm2d-22           [-1, 40, 16, 16]              80
             ReLU-23           [-1, 40, 16, 16]               0
           Conv2d-24           [-1, 40, 16, 16]          14,400
      BatchNorm2d-25           [-1, 40, 16, 16]              80
             ReLU-26           [-1, 40, 16, 16]               0
       BasicBlock-27           [-1, 40, 16, 16]               0
           Conv2d-28             [-1, 80, 8, 8]          28,800
      BatchNorm2d-29             [-1, 80, 8, 8]             160
             ReLU-30             [-1, 80, 8, 8]               0
           Conv2d-31             [-1, 80, 8, 8]          57,600
      BatchNorm2d-32             [-1, 80, 8, 8]             160
           Conv2d-33             [-1, 80, 8, 8]           3,200
      BatchNorm2d-34             [-1, 80, 8, 8]             160
             ReLU-35             [-1, 80, 8, 8]               0
       BasicBlock-36             [-1, 80, 8, 8]               0
           Conv2d-37             [-1, 80, 8, 8]          57,600
      BatchNorm2d-38             [-1, 80, 8, 8]             160
             ReLU-39             [-1, 80, 8, 8]               0
           Conv2d-40             [-1, 80, 8, 8]          57,600
      BatchNorm2d-41             [-1, 80, 8, 8]             160
             ReLU-42             [-1, 80, 8, 8]               0
       BasicBlock-43             [-1, 80, 8, 8]               0
           Conv2d-44             [-1, 80, 8, 8]          57,600
      BatchNorm2d-45             [-1, 80, 8, 8]             160
             ReLU-46             [-1, 80, 8, 8]               0
           Conv2d-47             [-1, 80, 8, 8]          57,600
      BatchNorm2d-48             [-1, 80, 8, 8]             160
             ReLU-49             [-1, 80, 8, 8]               0
       BasicBlock-50             [-1, 80, 8, 8]               0
           Conv2d-51            [-1, 120, 8, 8]          86,400
      BatchNorm2d-52            [-1, 120, 8, 8]             240
             ReLU-53            [-1, 120, 8, 8]               0
           Conv2d-54            [-1, 120, 8, 8]         129,600
      BatchNorm2d-55            [-1, 120, 8, 8]             240
           Conv2d-56            [-1, 120, 8, 8]           9,600
      BatchNorm2d-57            [-1, 120, 8, 8]             240
             ReLU-58            [-1, 120, 8, 8]               0
       BasicBlock-59            [-1, 120, 8, 8]               0
           Conv2d-60            [-1, 120, 8, 8]         129,600
      BatchNorm2d-61            [-1, 120, 8, 8]             240
             ReLU-62            [-1, 120, 8, 8]               0
           Conv2d-63            [-1, 120, 8, 8]         129,600
      BatchNorm2d-64            [-1, 120, 8, 8]             240
             ReLU-65            [-1, 120, 8, 8]               0
       BasicBlock-66            [-1, 120, 8, 8]               0
           Conv2d-67            [-1, 120, 4, 4]         129,600
      BatchNorm2d-68            [-1, 120, 4, 4]             240
             ReLU-69            [-1, 120, 4, 4]               0
           Conv2d-70            [-1, 120, 4, 4]         129,600
      BatchNorm2d-71            [-1, 120, 4, 4]             240
           Conv2d-72            [-1, 120, 4, 4]          14,400
      BatchNorm2d-73            [-1, 120, 4, 4]             240
             ReLU-74            [-1, 120, 4, 4]               0
       BasicBlock-75            [-1, 120, 4, 4]               0
           Conv2d-76            [-1, 140, 4, 4]         151,200
      BatchNorm2d-77            [-1, 140, 4, 4]             280
             ReLU-78            [-1, 140, 4, 4]               0
           Conv2d-79            [-1, 140, 4, 4]         176,400
      BatchNorm2d-80            [-1, 140, 4, 4]             280
           Conv2d-81            [-1, 140, 4, 4]          16,800
      BatchNorm2d-82            [-1, 140, 4, 4]             280
             ReLU-83            [-1, 140, 4, 4]               0
       BasicBlock-84            [-1, 140, 4, 4]               0
           Conv2d-85            [-1, 140, 4, 4]         176,400
      BatchNorm2d-86            [-1, 140, 4, 4]             280
             ReLU-87            [-1, 140, 4, 4]               0
           Conv2d-88            [-1, 140, 4, 4]         176,400
      BatchNorm2d-89            [-1, 140, 4, 4]             280
             ReLU-90            [-1, 140, 4, 4]               0
       BasicBlock-91            [-1, 140, 4, 4]               0
           Conv2d-92            [-1, 140, 4, 4]         176,400
      BatchNorm2d-93            [-1, 140, 4, 4]             280
             ReLU-94            [-1, 140, 4, 4]               0
           Conv2d-95            [-1, 140, 4, 4]         176,400
      BatchNorm2d-96            [-1, 140, 4, 4]             280
             ReLU-97            [-1, 140, 4, 4]               0
       BasicBlock-98            [-1, 140, 4, 4]               0
           Conv2d-99            [-1, 160, 4, 4]         201,600
     BatchNorm2d-100            [-1, 160, 4, 4]             320
            ReLU-101            [-1, 160, 4, 4]               0
          Conv2d-102            [-1, 160, 4, 4]         230,400
     BatchNorm2d-103            [-1, 160, 4, 4]             320
          Conv2d-104            [-1, 160, 4, 4]          22,400
     BatchNorm2d-105            [-1, 160, 4, 4]             320
            ReLU-106            [-1, 160, 4, 4]               0
      BasicBlock-107            [-1, 160, 4, 4]               0
          Conv2d-108            [-1, 160, 4, 4]         230,400
     BatchNorm2d-109            [-1, 160, 4, 4]             320
            ReLU-110            [-1, 160, 4, 4]               0
          Conv2d-111            [-1, 160, 4, 4]         230,400
     BatchNorm2d-112            [-1, 160, 4, 4]             320
            ReLU-113            [-1, 160, 4, 4]               0
      BasicBlock-114            [-1, 160, 4, 4]               0
          Conv2d-115            [-1, 160, 2, 2]         230,400
     BatchNorm2d-116            [-1, 160, 2, 2]             320
            ReLU-117            [-1, 160, 2, 2]               0
          Conv2d-118            [-1, 160, 2, 2]         230,400
     BatchNorm2d-119            [-1, 160, 2, 2]             320
          Conv2d-120            [-1, 160, 2, 2]          25,600
     BatchNorm2d-121            [-1, 160, 2, 2]             320
            ReLU-122            [-1, 160, 2, 2]               0
      BasicBlock-123            [-1, 160, 2, 2]               0
          Conv2d-124            [-1, 180, 2, 2]         259,200
     BatchNorm2d-125            [-1, 180, 2, 2]             360
            ReLU-126            [-1, 180, 2, 2]               0
          Conv2d-127            [-1, 180, 2, 2]         291,600
     BatchNorm2d-128            [-1, 180, 2, 2]             360
          Conv2d-129            [-1, 180, 2, 2]          28,800
     BatchNorm2d-130            [-1, 180, 2, 2]             360
            ReLU-131            [-1, 180, 2, 2]               0
      BasicBlock-132            [-1, 180, 2, 2]               0
          Conv2d-133            [-1, 200, 2, 2]         324,000
     BatchNorm2d-134            [-1, 200, 2, 2]             400
            ReLU-135            [-1, 200, 2, 2]               0
          Conv2d-136            [-1, 200, 2, 2]         360,000
     BatchNorm2d-137            [-1, 200, 2, 2]             400
          Conv2d-138            [-1, 200, 2, 2]          36,000
     BatchNorm2d-139            [-1, 200, 2, 2]             400
            ReLU-140            [-1, 200, 2, 2]               0
      BasicBlock-141            [-1, 200, 2, 2]               0
AdaptiveAvgPool2d-142            [-1, 200, 1, 1]               0
          Linear-143                   [-1, 10]           2,010
================================================================
Total params: 4,924,990
Trainable params: 4,924,990
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 5.25
Params size (MB): 18.79
Estimated Total Size (MB): 24.04
----------------------------------------------------------------
Summary second time None
[EPOCH: 1,   100] loss: 2.084
[EPOCH: 1,   200] loss: 1.774
[EPOCH: 1,   300] loss: 1.677
Accuracy on test set: 34 %
[EPOCH: 2,   100] loss: 1.532
[EPOCH: 2,   200] loss: 1.475
[EPOCH: 2,   300] loss: 1.436
Accuracy on test set: 45 %
[EPOCH: 3,   100] loss: 1.356
[EPOCH: 3,   200] loss: 1.307
[EPOCH: 3,   300] loss: 1.280
Accuracy on test set: 56 %
[EPOCH: 4,   100] loss: 1.173
[EPOCH: 4,   200] loss: 1.148
[EPOCH: 4,   300] loss: 1.116
Accuracy on test set: 55 %
[EPOCH: 5,   100] loss: 1.052
[EPOCH: 5,   200] loss: 0.994
[EPOCH: 5,   300] loss: 0.992
Accuracy on test set: 56 %
[EPOCH: 6,   100] loss: 0.906
[EPOCH: 6,   200] loss: 0.890
[EPOCH: 6,   300] loss: 0.886
Accuracy on test set: 68 %
[EPOCH: 7,   100] loss: 0.811
[EPOCH: 7,   200] loss: 0.802
[EPOCH: 7,   300] loss: 0.808
Accuracy on test set: 63 %
[EPOCH: 8,   100] loss: 0.779
[EPOCH: 8,   200] loss: 0.756
[EPOCH: 8,   300] loss: 0.743
Accuracy on test set: 72 %
[EPOCH: 9,   100] loss: 0.717
[EPOCH: 9,   200] loss: 0.711
[EPOCH: 9,   300] loss: 0.708
Accuracy on test set: 73 %
[EPOCH: 10,   100] loss: 0.666
[EPOCH: 10,   200] loss: 0.685
[EPOCH: 10,   300] loss: 0.670
Accuracy on test set: 76 %
[EPOCH: 11,   100] loss: 0.649
[EPOCH: 11,   200] loss: 0.666
[EPOCH: 11,   300] loss: 0.639
Accuracy on test set: 72 %
[EPOCH: 12,   100] loss: 0.629
[EPOCH: 12,   200] loss: 0.642
[EPOCH: 12,   300] loss: 0.643
Accuracy on test set: 76 %
[EPOCH: 13,   100] loss: 0.595
[EPOCH: 13,   200] loss: 0.619
[EPOCH: 13,   300] loss: 0.596
Accuracy on test set: 75 %
[EPOCH: 14,   100] loss: 0.584
[EPOCH: 14,   200] loss: 0.594
[EPOCH: 14,   300] loss: 0.604
Accuracy on test set: 75 %
[EPOCH: 15,   100] loss: 0.605
[EPOCH: 15,   200] loss: 0.584
[EPOCH: 15,   300] loss: 0.611
Accuracy on test set: 76 %
[EPOCH: 16,   100] loss: 0.576
[EPOCH: 16,   200] loss: 0.586
[EPOCH: 16,   300] loss: 0.567
Accuracy on test set: 74 %
[EPOCH: 17,   100] loss: 0.559
[EPOCH: 17,   200] loss: 0.555
[EPOCH: 17,   300] loss: 0.581
Accuracy on test set: 75 %
[EPOCH: 18,   100] loss: 0.556
[EPOCH: 18,   200] loss: 0.566
[EPOCH: 18,   300] loss: 0.558
Accuracy on test set: 76 %
[EPOCH: 19,   100] loss: 0.548
[EPOCH: 19,   200] loss: 0.564
[EPOCH: 19,   300] loss: 0.545
Accuracy on test set: 78 %
[EPOCH: 20,   100] loss: 0.559
[EPOCH: 20,   200] loss: 0.554
[EPOCH: 20,   300] loss: 0.539
Accuracy on test set: 75 %
[EPOCH: 21,   100] loss: 0.534
[EPOCH: 21,   200] loss: 0.537
[EPOCH: 21,   300] loss: 0.554
Accuracy on test set: 77 %
[EPOCH: 22,   100] loss: 0.528
[EPOCH: 22,   200] loss: 0.542
[EPOCH: 22,   300] loss: 0.535
Accuracy on test set: 79 %
[EPOCH: 23,   100] loss: 0.514
[EPOCH: 23,   200] loss: 0.526
[EPOCH: 23,   300] loss: 0.531
Accuracy on test set: 77 %
[EPOCH: 24,   100] loss: 0.515
[EPOCH: 24,   200] loss: 0.502
[EPOCH: 24,   300] loss: 0.513
Accuracy on test set: 80 %
[EPOCH: 25,   100] loss: 0.488
[EPOCH: 25,   200] loss: 0.462
[EPOCH: 25,   300] loss: 0.470
Accuracy on test set: 80 %
[EPOCH: 26,   100] loss: 0.468
[EPOCH: 26,   200] loss: 0.450
[EPOCH: 26,   300] loss: 0.471
Accuracy on test set: 81 %
[EPOCH: 27,   100] loss: 0.420
[EPOCH: 27,   200] loss: 0.449
[EPOCH: 27,   300] loss: 0.434
Accuracy on test set: 82 %
[EPOCH: 28,   100] loss: 0.406
[EPOCH: 28,   200] loss: 0.418
[EPOCH: 28,   300] loss: 0.434
Accuracy on test set: 80 %
[EPOCH: 29,   100] loss: 0.395
[EPOCH: 29,   200] loss: 0.407
[EPOCH: 29,   300] loss: 0.405
Accuracy on test set: 77 %
[EPOCH: 30,   100] loss: 0.376
[EPOCH: 30,   200] loss: 0.372
[EPOCH: 30,   300] loss: 0.372
Accuracy on test set: 82 %
[EPOCH: 31,   100] loss: 0.365
[EPOCH: 31,   200] loss: 0.364
[EPOCH: 31,   300] loss: 0.373
Accuracy on test set: 84 %
[EPOCH: 32,   100] loss: 0.337
[EPOCH: 32,   200] loss: 0.343
[EPOCH: 32,   300] loss: 0.347
Accuracy on test set: 82 %
[EPOCH: 33,   100] loss: 0.324
[EPOCH: 33,   200] loss: 0.320
[EPOCH: 33,   300] loss: 0.321
Accuracy on test set: 85 %
[EPOCH: 34,   100] loss: 0.287
[EPOCH: 34,   200] loss: 0.308
[EPOCH: 34,   300] loss: 0.315
Accuracy on test set: 85 %
[EPOCH: 35,   100] loss: 0.274
[EPOCH: 35,   200] loss: 0.289
[EPOCH: 35,   300] loss: 0.286
Accuracy on test set: 85 %
[EPOCH: 36,   100] loss: 0.259
[EPOCH: 36,   200] loss: 0.278
[EPOCH: 36,   300] loss: 0.256
Accuracy on test set: 87 %
[EPOCH: 37,   100] loss: 0.234
[EPOCH: 37,   200] loss: 0.254
[EPOCH: 37,   300] loss: 0.251
Accuracy on test set: 87 %
[EPOCH: 38,   100] loss: 0.203
[EPOCH: 38,   200] loss: 0.224
[EPOCH: 38,   300] loss: 0.231
Accuracy on test set: 88 %
[EPOCH: 39,   100] loss: 0.197
[EPOCH: 39,   200] loss: 0.209
[EPOCH: 39,   300] loss: 0.197
Accuracy on test set: 88 %
[EPOCH: 40,   100] loss: 0.181
[EPOCH: 40,   200] loss: 0.173
[EPOCH: 40,   300] loss: 0.171
Accuracy on test set: 88 %
[EPOCH: 41,   100] loss: 0.165
[EPOCH: 41,   200] loss: 0.163
[EPOCH: 41,   300] loss: 0.155
Accuracy on test set: 89 %
[EPOCH: 42,   100] loss: 0.133
[EPOCH: 42,   200] loss: 0.135
[EPOCH: 42,   300] loss: 0.139
Accuracy on test set: 90 %
[EPOCH: 43,   100] loss: 0.118
[EPOCH: 43,   200] loss: 0.118
[EPOCH: 43,   300] loss: 0.109
Accuracy on test set: 90 %
[EPOCH: 44,   100] loss: 0.096
[EPOCH: 44,   200] loss: 0.093
[EPOCH: 44,   300] loss: 0.091
Accuracy on test set: 90 %
[EPOCH: 45,   100] loss: 0.079
[EPOCH: 45,   200] loss: 0.078
[EPOCH: 45,   300] loss: 0.085
Accuracy on test set: 91 %
[EPOCH: 46,   100] loss: 0.073
[EPOCH: 46,   200] loss: 0.081
[EPOCH: 46,   300] loss: 0.075
Accuracy on test set: 90 %
[EPOCH: 47,   100] loss: 0.067
[EPOCH: 47,   200] loss: 0.079
[EPOCH: 47,   300] loss: 0.074
Accuracy on test set: 90 %
[EPOCH: 48,   100] loss: 0.067
[EPOCH: 48,   200] loss: 0.069
[EPOCH: 48,   300] loss: 0.071
Accuracy on test set: 91 %
[EPOCH: 49,   100] loss: 0.063
[EPOCH: 49,   200] loss: 0.062
[EPOCH: 49,   300] loss: 0.065
Accuracy on test set: 90 %
[EPOCH: 50,   100] loss: 0.059
[EPOCH: 50,   200] loss: 0.055
[EPOCH: 50,   300] loss: 0.061
Accuracy on test set: 90 %
[EPOCH: 51,   100] loss: 0.051
[EPOCH: 51,   200] loss: 0.062
[EPOCH: 51,   300] loss: 0.063
Accuracy on test set: 91 %
[EPOCH: 52,   100] loss: 0.056
[EPOCH: 52,   200] loss: 0.056
[EPOCH: 52,   300] loss: 0.052
Accuracy on test set: 91 %
[EPOCH: 53,   100] loss: 0.052
[EPOCH: 53,   200] loss: 0.053
[EPOCH: 53,   300] loss: 0.052
Accuracy on test set: 91 %
[EPOCH: 54,   100] loss: 0.052
[EPOCH: 54,   200] loss: 0.047
[EPOCH: 54,   300] loss: 0.050
Accuracy on test set: 91 %
[EPOCH: 55,   100] loss: 0.047
[EPOCH: 55,   200] loss: 0.046
[EPOCH: 55,   300] loss: 0.047
Accuracy on test set: 91 %
