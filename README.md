# Classifying CIFAR-10 Dataset Using a Modified Residual Network (ResNet)

Hey! Welcome to the repository for our Deep Learning mini-project completed by Anubhav Ghildiyal (ag8766), Noel Nebu Panicker (nnp5666) and Yogya Sharma (ys5250).

## Objective:

In this project we have designed and trained 3 different modifited ResNet architectures on CIFAR-10 dataset. The goal is to learn a model that achieves highest test accuracy under the constraint that the number of trainable parameters be less than 5M.

### Architecture 1
In this model, the number of parameters were modified by just updating the number of input and output channels of each layer of the model.
The num_hidden parameter was set to 39 (basically updated the input channels from 64 to 39), which in turn updated the input and output channels for each layer. We were successfully able to reduce the number of parameters to 4.4 million while keeping the number of layers same.
Here, the number of basic blocks in each layer was kept same, so each layer has 2 basic blocks. Since each basic block has 2 convolutional layers, the number of convolutional layers in the main forward path is 17,
including the first convolutional layer same as the stock ResNet-18 model.
Here we were able to achieve an accuracy of 85%, after running the model for 55 epochs, which is below the requirement of the project. Moreover, by just reducing the number of input channels resulted in the reduction of the networks ability to learn fine-grained features and patterns of the images. Reducing the output channels reduced the number of feature maps generated by the network in each convolutional layer, resulting in a decrease in the network's ability to capture diverse and discriminative features from the input images. The reduced output channels limited the network's representational capacity and hindered its ability to model complex patterns and variations in the data. So, just updating the number of input/output channels without changing other features of the model resulted in an underperforming model, which took us to a new approach, described in the architecture below.

Number of trainable parameters: *4.42M*

### Architecture 2
In this model, we essentially wan ted to develop a deep model and created a 12-layer network. The num_hidden parameter was set to 20 (basically updated the input channels from 64 to 20), which in turn updated the input and output channels for each layer. We were successfully able to reduce the number of parameters to 4,924,990 by updating the structure of the network.
In this architecture, we have modified the number of basic blocks in each layer. For the 12 layers, following are the number of basic blocks in each layer: [2,1,2,1,2,1,2,1,2,1,1,1]. Since each basic block has 2 convolutional layers, the number of convolutional layers in the main forward path is 35, including the first convolutional layer.
By making the network deeper, we improved its ability to learn complex representations from the data. The trade off being that reducing the number of input and output channels in ResNet-18 resulted in a decrease in the network's ability to capture and represent complex features, which reduced its performance in the image recognition task, while still being better than the previous architecture.
We were able to achieve an accuracy of 91%, which is acceptable, but we wanted to search for a better model. So, we explored working with decreasing the number of residual layers in the model, while increasing the number of input channels for the convolutional layers.

Number of trainable parameters: *4.92M*

### Architecture 3
In this model, the number of parameters were updated by updating the number of input and output channels of each layer of the model and also by increasing the number of residual blocks in the network as compared to the stock ResNet18 model, hence making it deeper. We brought down the number of layers to 8, reducing it from 12 discussed in the previous architecture. The num_hidden parameter was set to 32 (basically updated the input channels from 64 to 32), which in turn updated the input and output channels for each layer. We were successfully able to reduce the number of parameters to 4,999,786 by updating the structure of the network.
In this architecture, we have modified the number of basic blocks in each layer. For the 8 layers, following are the number of basic blocks in each layer: [1,1,2,1,1,2,1,1].
Since each basic block has 2 convolutional layers, the number of convolutional layers in the main forward path is 21, including the first convolutional layer.
In this architecture, we have reduced the total number of convolutional layers from Architecture-2 but increased the number of input channels to each convolutional layer. This reduces the risk of overfitting and enhances the modelâ€™s ability to learn complex representations from the data.
We were able to achieve an accuracy of 92%, which is above the benchmark value of 90% for this project.

Number of trainable parameters: *4.99M*

## Results:
Architecture 3 achieved best test accuracy of 92%.


![WhatsApp Image 2023-04-15 at 22 03 51](https://github.com/NoelNebu2206/DLS23_miniproj/assets/115520834/69239706-dccd-4cba-94e3-0b7a9d1846fc)

![WhatsApp Image 2023-04-15 at 22 06 06](https://github.com/NoelNebu2206/DLS23_miniproj/assets/115520834/be650be4-88fd-45b0-ad7a-c786e6ea7de7)

